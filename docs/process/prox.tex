
The proximity classifier learns the patterns of symbols present in the data set,
and then classifies any unknown symbols using the surrounding symbols, and its
knowledge of the patterns and relationships of the symbols. Here we begin by
outlining the methodology used; we want to classify the symbol $x$ at a given
position, as Figure~\ref{fig:neighbours} illustrates.

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[node distance=2cm, auto]
    \node[dcell](top){D};
    \node[ucell, below of=top](center){$X$~?};
    \node[fcell, left of=center](left){F};
    \node[gcell, right of=center](right){G};
    \node[gcell, below of=center](bottom){G};

    \path[line] (top)   -- node[right] {} (center);
    \path[line] (left)   -- node[above] {} (center);
    \path[line] (right)   -- node[above] {} (center);
    \path[line] (bottom)   -- node[right] {} (center);
\end{tikzpicture}
\end{center}
\caption{This example is from cell 2,19 in Pallet Town, Note: $G$=grass, $D$=dirt, $F$=forest}
\label{fig:neighbours}
\end{figure}


\begin{table}[h]
\label{table:relativefreq}
\begin{center}
\begin{tabular}{ l | r r r r r r }
              & $c_{B}$& $c_{D}$& $c_{EDGE}$& $c_{F}$& $c_{G}$& $c_{W}$ \\ 
              \hline
    north     & 0      & 0.1558 & 0.0519    & 0.0519 & 0.7403 & 0.0325\\
    east      & 0.0779 & 0.0974 & 0         & 0.1493 & 0.6428 & 0\\
    south     & 0      & 0.2403 & 0.0130    & 0.0065 & 0.7403 & 0\\
    west      & 0.0519 & 0.2273 & 0         & 0.0519 & 0.6428 & 0.0260\\
\end{tabular}
\caption{Probability of the neighbours for a grass cell in Pallet Town}
\end{center}
\end{table}

To classify x, we first calculate the probability that $x$ belongs to each
class, $P(X)\quad X_i = P(x\!\in\! C_i)$. The proximity classifier is given the
class of each symbol in the cardinal directions: north, south, east, and west.
Using this information, we restrict the probability calculation to consider
only the case where a symbol has the given classes neighbouring it. Such a
restriction is the conditional probability:

\[
P(X) = P(X|\,N\!=\!c_1,\,E\!=\!c_2,\,S\!=\!c_3,\,W\!=\!c_4)
\]

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[node distance=3cm, auto]
    \node[ccell](top){$C_{i,j+1}$};
    \node[ucell, below=of top](center){$X_{i,j}$};
    \node[ccell, left=of center](left){$C_{i-1,j}$};
    \node[ccell, right=of center](right){$C_{i+1,j}$};
    \node[ccell, below=of center](bottom){$C_{i,j-1}$};

    \path[line] (top)   -- node[above right] {$P(X | \, N=c_{1})$} (center);
    \path[line] (left)  -- node[above] {$P(X | \, W=c_{2})$} (center);
    \path[line] (right) -- node[above] {$P(X | \, E=c_{3})$} (center);
    \path[line] (bottom) -- node[below right] {$P(X | \, S=c_{4})$} (center);
\end{tikzpicture}
\end{center}
\caption{The conditional probability of the classes for $X_{i,j}$, given the neighbouring cells}
\label{fig:conditionalprob}
\end{figure}


In calculating probabilities, we can only approximate them using relative
frequencies collected from our sample data. For a set of $k$ classes, there are
$k^5$ possible combinations of cells, 4 neighbouring cells and the centre, and
too many of these combinations occur rarely in our dataset to make confident
approximations of the true probabilities. To resolve this problem, we assume
that the conditional probabilities are independent in each direction. Under
this assumption, the calculation becomes much more reasonable for our small
data set, since there are only $4k$ pieces of data to record for each symbol.
Our resulting calculation is given in Equation~\ref{eq:prox}.

\begin{equation}
\label{eq:prox}
P(X) \approx P(X|\,N\!=\!c_1)P(X|\,S\!=\!c_2)P(X|\,E\!=\!c_3)P(X|\,W\!=\!c_4)
\end{equation}

From the definition of conditional probability we have that for some class $c$ in
the direction $D$:
\[
P(X|\,D\!=\!c) = \frac{P(X \cap D\!=\!c)}{P(D=c)}
\]

To calculate each of the conditional probabilities, we compute a table from the
samples recording $P(X\cap D\!=\!c)$ and $P(D\!=\!c)$ for each class  $c$, and
direction $D$, an example is given for a single class in
Table~\ref{table:relativefreq}.  We use the table to calculate the relative
conditional frequencies and, under the assumption of independence, we
approximate $P(X)$ using Equation~\ref{eq:prox}.

Having the probability vector $P(X)$ we create an input file for WEKA.  This
file includes the probability vector, the neighbouring classes, and the class
of the symbol being classified for training. WEKA creates a decision tree which
is used to reduce the error in the probability calculation that is introduced
under the assumption of independence.

